{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eMMJssyxQ-J"
      },
      "outputs": [],
      "source": [
        "#!pip install --upgrade tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C24JKO7LxP3O"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf### models\n",
        "import numpy as np### math computations\n",
        "import matplotlib.pyplot as plt### plotting bar chart\n",
        "import sklearn### machine learning library\n",
        "from sklearn.metrics import confusion_matrix, roc_curve### metrics\n",
        "import seaborn as sns### visualizations\n",
        "import datetime\n",
        "import pathlib\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "from numpy import random\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import (Dense,Flatten,InputLayer,LayerNormalization,Dropout,Input,MultiHeadAttention,Embedding,TextVectorization)\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import Accuracy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
        "from google.colab import drive\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LdwxalEeyUX5",
        "outputId": "5015fd6d-7b61-4382-af4f-36d2242cb854"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.15.0'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKg0HdsrYA0S"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgU1Z8fcOSW_"
      },
      "source": [
        "## Data Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hwhov2uvDdo",
        "outputId": "294898a5-bc92-46af-9429-16f884beec40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-01-27 15:12:23--  https://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7833145 (7.5M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip’\n",
            "\n",
            "fra-eng.zip         100%[===================>]   7.47M  20.7MB/s    in 0.4s    \n",
            "\n",
            "2024-01-27 15:12:23 (20.7 MB/s) - ‘fra-eng.zip’ saved [7833145/7833145]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.manythings.org/anki/fra-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-ufBPwn8_ax",
        "outputId": "e6933b07-8b35-440b-911a-29f1fc31e2fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/fra-eng.zip\n",
            "  inflating: /content/dataset/_about.txt  \n",
            "  inflating: /content/dataset/fra.txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip \"/content/fra-eng.zip\" -d \"/content/dataset/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omyb2Dq_YHbT"
      },
      "source": [
        "## Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywC7WnoIJeXl"
      },
      "outputs": [],
      "source": [
        "# Load the text data from a file into a tf.data.TextLineDataset object\n",
        "text_dataset=tf.data.TextLineDataset(\"/content/dataset/fra.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlcJxz6cJeb_"
      },
      "outputs": [],
      "source": [
        "# Define constants for vocabulary size, sequence lengths, embedding dimensions, and batch size\n",
        "VOCAB_SIZE=20000\n",
        "ENGLISH_SEQUENCE_LENGTH=32\n",
        "FRENCH_SEQUENCE_LENGTH=32\n",
        "EMBEDDING_DIM=256\n",
        "BATCH_SIZE=128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TePer9o-JeeR"
      },
      "outputs": [],
      "source": [
        "# Create a TextVectorization layer for English and French with specified parameters\n",
        "english_vectorize_layer = TextVectorization(\n",
        "    standardize='lower_and_strip_punctuation',  # Convert text to lowercase and strip punctuation\n",
        "    max_tokens=VOCAB_SIZE,                      # Set the maximum number of tokens in the vocabulary\n",
        "    output_mode='int',                         # Output integer indices for tokens\n",
        "    output_sequence_length=ENGLISH_SEQUENCE_LENGTH  # Pad or truncate sequences to this length\n",
        ")\n",
        "\n",
        "\n",
        "french_vectorize_layer = TextVectorization(\n",
        "    standardize='lower_and_strip_punctuation',  # Convert text to lowercase and strip punctuation\n",
        "    max_tokens=VOCAB_SIZE,                      # Set the maximum number of tokens in the vocabulary\n",
        "    output_mode='int',                         # Output integer indices for tokens\n",
        "    output_sequence_length=FRENCH_SEQUENCE_LENGTH  # Pad or truncate sequences to this length\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNK3uewuQTjY"
      },
      "outputs": [],
      "source": [
        "# Function to split the text into input and output sequences\n",
        "def selector(input_text):\n",
        "  split_text=tf.strings.split(input_text,'\\t')\n",
        "  return {'input_1':split_text[0:1],'input_2':'starttoken '+split_text[1:2]},split_text[1:2]+' endtoken'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fcSDED_2c-z"
      },
      "outputs": [],
      "source": [
        "# Function to separate text into input and output sequences for training\n",
        "def separator(input_text):\n",
        "    split_text = tf.strings.split(input_text, '\\t')  # Split text by tab\n",
        "    # Return input and output sequences\n",
        "    return split_text[0:1], 'starttoken ' + split_text[1:2] + ' endtoken'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktN95hiN2pgU"
      },
      "outputs": [],
      "source": [
        "# Apply the selector function to the text dataset\n",
        "split_dataset = text_dataset.map(selector)\n",
        "\n",
        "# Create an initial dataset for English and French sequences\n",
        "init_dataset = text_dataset.map(separator)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQkSkpd1tils",
        "outputId": "37fe07de-4f12-49cf-f4ef-39a91027cba2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "({'input_1': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, 'input_2': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken Va !'], dtype=object)>}, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Va ! endtoken'], dtype=object)>)\n",
            "({'input_1': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, 'input_2': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken Marche.'], dtype=object)>}, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Marche. endtoken'], dtype=object)>)\n",
            "({'input_1': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, 'input_2': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken En route !'], dtype=object)>}, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'En route ! endtoken'], dtype=object)>)\n"
          ]
        }
      ],
      "source": [
        "# samples\n",
        "for i in split_dataset.take(3):\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_usEDYiOJeil"
      },
      "outputs": [],
      "source": [
        "english_training_data=init_dataset.map(lambda x,y:x)### input x,y and output x\n",
        "english_vectorize_layer.adapt(english_training_data)#### adapt the vectorize_layer to the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dl7pxJprJek2"
      },
      "outputs": [],
      "source": [
        "french_training_data=init_dataset.map(lambda x,y:y)### input x,y and output y\n",
        "french_vectorize_layer.adapt(french_training_data)#### adapt the vectorize_layer to the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yVIMxvTJemt"
      },
      "outputs": [],
      "source": [
        "# Function to apply vectorization to the dataset\n",
        "def vectorizer(inputs,output):\n",
        "  return {'input_1':english_vectorize_layer(inputs['input_1']),\n",
        "          'input_2':french_vectorize_layer(inputs['input_2'])},french_vectorize_layer(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wI9-GPpVJepF"
      },
      "outputs": [],
      "source": [
        "# Apply the vectorizer function to the split dataset\n",
        "dataset=split_dataset.map(vectorizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_6Xtks8wPk5",
        "outputId": "c2ad6e12-bb79-445b-8442-788331bcdc9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "({'input_1': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, 'input_2': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken Va !'], dtype=object)>}, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Va ! endtoken'], dtype=object)>)\n",
            "({'input_1': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, 'input_2': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken Marche.'], dtype=object)>}, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Marche. endtoken'], dtype=object)>)\n",
            "({'input_1': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, 'input_2': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken En route !'], dtype=object)>}, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'En route ! endtoken'], dtype=object)>)\n"
          ]
        }
      ],
      "source": [
        "# sample\n",
        "for i in split_dataset.take(3):\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FM9aenklufpC",
        "outputId": "5f663293-95e8-43b9-c6a9-2e79b8a6dcca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "({'input_1': <tf.Tensor: shape=(1, 32), dtype=int64, numpy=\n",
            "array([[44,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])>, 'input_2': <tf.Tensor: shape=(1, 32), dtype=int64, numpy=\n",
            "array([[  2, 103,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0]])>}, <tf.Tensor: shape=(1, 32), dtype=int64, numpy=\n",
            "array([[103,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0]])>)\n"
          ]
        }
      ],
      "source": [
        "# sample\n",
        "for i in dataset.take(1):\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WE0s6p9TiM9"
      },
      "outputs": [],
      "source": [
        "# Shuffle, unbatch, batch, and prefetch the dataset for training\n",
        "dataset=dataset.shuffle(2048).unbatch().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5JXDdrNtwRj"
      },
      "outputs": [],
      "source": [
        "NUM_BATCHES = int(200000/BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQvg18V5TiO7"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and validation sets\n",
        "train_dataset = dataset.take(int(0.9 * NUM_BATCHES))  # 90% for training\n",
        "val_dataset = dataset.skip(int(0.9 * NUM_BATCHES))  # 10% for validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBVM2EB0Xh97"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXZLSM5pkS3w"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1_5MCwq0ARk"
      },
      "outputs": [],
      "source": [
        "# Purpose: Computes positional encodings for sequences to provide information about token positions, necessary for models like Transformers that do not have inherent position-awareness.\n",
        "\n",
        "def positional_encoding(model_size, SEQUENCE_LENGTH):\n",
        "  \"\"\"\n",
        "  Computes positional encoding for a sequence of given length and model size.\n",
        "\n",
        "  Args:\n",
        "      model_size (int): Dimensionality of the positional encoding vectors (embedding size).\n",
        "      SEQUENCE_LENGTH (int): Length of the sequence for which positional encodings are computed.\n",
        "\n",
        "  Returns:\n",
        "      tf.Tensor: A tensor of shape (1, SEQUENCE_LENGTH, model_size) containing the positional encodings.\n",
        "  \"\"\"\n",
        "  \n",
        "  output = []  # List to store positional encoding vectors for each position\n",
        "\n",
        "  for pos in range(SEQUENCE_LENGTH):\n",
        "      PE = np.zeros((model_size))  # Initialize a vector for the current position with zeros\n",
        "      \n",
        "      for i in range(model_size):\n",
        "          if i % 2 == 0:\n",
        "              # For even indices, use the sine function\n",
        "              PE[i] = np.sin(pos / (10000 ** (i / model_size)))\n",
        "          else:\n",
        "              # For odd indices, use the cosine function\n",
        "              PE[i] = np.cos(pos / (10000 ** ((i - 1) / model_size)))\n",
        "      \n",
        "      output.append(tf.expand_dims(PE, axis=0))  # Add the positional encoding vector to the list, expanding dims for batch dimension\n",
        "\n",
        "  out = tf.concat(output, axis=0)  # Concatenate all positional encoding vectors along the first axis (sequence length)\n",
        "  out = tf.expand_dims(out, axis=0)  # Add an additional dimension for the batch size (batch dimension)\n",
        "\n",
        "  return tf.cast(out, dtype=tf.float32)  # Cast the output tensor to float32 and return it\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PD4gK6uLxY8B",
        "outputId": "bbd59e58-7749-4ba0-83c6-0385a76211c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 64, 256)\n"
          ]
        }
      ],
      "source": [
        "print(positional_encoding(256,64).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZM8tUhJ9G0tH"
      },
      "outputs": [],
      "source": [
        "# The Embeddings class combines token embeddings with positional encodings, providing enriched token representations with positional information for sequence data.\n",
        "\n",
        "class Embeddings(Layer):\n",
        "    \"\"\"\n",
        "    Custom embedding layer that combines token embeddings with positional encodings.\n",
        "\n",
        "    Args:\n",
        "        sequence_length (int): Length of the input sequences.\n",
        "        vocab_size (int): Size of the vocabulary (number of unique tokens).\n",
        "        embed_dim (int): Dimensionality of the embedding vectors.\n",
        "    \n",
        "    Attributes:\n",
        "        token_embeddings (tf.keras.layers.Embedding): Layer for token embeddings.\n",
        "        sequence_length (int): Length of the input sequences.\n",
        "        vocab_size (int): Size of the vocabulary.\n",
        "        embed_dim (int): Dimensionality of the embedding vectors.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim):\n",
        "        \"\"\"\n",
        "        Initialize the Embeddings layer.\n",
        "\n",
        "        Args:\n",
        "            sequence_length (int): Length of the input sequences.\n",
        "            vocab_size (int): Size of the vocabulary.\n",
        "            embed_dim (int): Dimensionality of the embedding vectors.\n",
        "        \"\"\"\n",
        "        super(Embeddings, self).__init__()\n",
        "        # Initialize the token embeddings layer\n",
        "        self.token_embeddings = Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Forward pass of the layer.\n",
        "\n",
        "        Args:\n",
        "            inputs (tf.Tensor): Input tensor containing token indices.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Sum of token embeddings and positional encodings.\n",
        "        \"\"\"\n",
        "        # Compute token embeddings for the input tokens\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        # Compute positional encodings based on embedding dimension and sequence length\n",
        "        embedded_positions = positional_encoding(\n",
        "            self.embed_dim, self.sequence_length)\n",
        "        # Add token embeddings and positional encodings to get the final embeddings\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        \"\"\"\n",
        "        Compute a mask for the input tensor, indicating valid positions.\n",
        "\n",
        "        Args:\n",
        "            inputs (tf.Tensor): Input tensor to compute the mask for.\n",
        "            mask (tf.Tensor, optional): Optional mask tensor (not used here).\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Boolean tensor where non-zero positions are marked as True.\n",
        "        \"\"\"\n",
        "        # Return a boolean mask indicating non-zero positions in the input tensor\n",
        "        return tf.math.not_equal(inputs, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sr0o-QcgD4xS",
        "outputId": "53b24182-8563-4740-8545-9ca6ece6f933"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 8, 512)\n"
          ]
        }
      ],
      "source": [
        "# testing\n",
        "test_input=tf.constant([[2,4,7,21,3,5,0,0]])\n",
        "emb=Embeddings(8,20000,512)\n",
        "emb_out=emb(test_input)\n",
        "print(emb_out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoZ59gSaom6P",
        "outputId": "e01e26c2-9d60-4ea5-ba5c-093e9cdb4bde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([[ True  True  True  True  True  True False False]], shape=(1, 8), dtype=bool)\n",
            "tf.Tensor(\n",
            "[[1 1 1 1 1 1 0 0]\n",
            " [1 1 1 1 1 1 0 0]\n",
            " [1 1 1 1 1 1 0 0]\n",
            " [1 1 1 1 1 1 0 0]\n",
            " [1 1 1 1 1 1 0 0]\n",
            " [1 1 1 1 1 1 0 0]\n",
            " [1 1 1 1 1 1 0 0]\n",
            " [1 1 1 1 1 1 0 0]], shape=(8, 8), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "# testing\n",
        "mask = emb.compute_mask(test_input)\n",
        "print(mask)\n",
        "\n",
        "\n",
        "padding_mask = tf.cast(\n",
        "    tf.repeat(mask,repeats=tf.shape(mask)[1],axis=0),\n",
        "    dtype=tf.int32)\n",
        "print(padding_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaDKmgJgklY_"
      },
      "source": [
        "## Custom MultiHeadAttention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIRfhrVINS39"
      },
      "outputs": [],
      "source": [
        "class CustomSelfAttention(Layer):\n",
        "  def __init__(self, model_size):\n",
        "      \"\"\"\n",
        "      Initialize the CustomSelfAttention layer.\n",
        "      \n",
        "      Args:\n",
        "          model_size (int): The size of the model (dimension of the query/key/value vectors).\n",
        "      \"\"\"\n",
        "      super(CustomSelfAttention, self).__init__()\n",
        "      self.model_size = model_size\n",
        "\n",
        "  def call(self, query, key, value, masking):\n",
        "      \"\"\"\n",
        "      Perform the self-attention operation.\n",
        "      \n",
        "      Args:\n",
        "          query (Tensor): Query matrix of shape (batch_size, seq_length, model_size).\n",
        "          key (Tensor): Key matrix of shape (batch_size, seq_length, model_size).\n",
        "          value (Tensor): Value matrix of shape (batch_size, seq_length, model_size).\n",
        "          masking (Tensor): Masking tensor of shape (batch_size, seq_length, seq_length), \n",
        "                            where masked positions are set to 0 and unmasked to 1.\n",
        "      \n",
        "      Returns:\n",
        "          Tensor: The output of the self-attention layer of shape (batch_size, seq_length, model_size).\n",
        "      \"\"\"\n",
        "      ######## Compute scores by performing matrix multiplication between query and key (transpose_b=True).\n",
        "      score = tf.matmul(query, key, transpose_b=True)\n",
        "      \n",
        "      ######## Scale the scores by dividing by the square root of the model size.\n",
        "      score /= tf.math.sqrt(tf.cast(self.model_size, tf.float32))\n",
        "      \n",
        "      ######## Apply masking by adding a large negative value (-1e10) to masked positions to ensure they are not attended to.\n",
        "      masking = tf.cast(masking, dtype=tf.float32)\n",
        "      score += (1. - masking) * -1e10\n",
        "      \n",
        "      ######## Compute attention weights using the softmax function, and apply masking.\n",
        "      attention = tf.nn.softmax(score, axis=-1) * masking\n",
        "      \n",
        "      ######## Compute the output by performing matrix multiplication between attention weights and value.\n",
        "      head = tf.matmul(attention, value)\n",
        "      \n",
        "      return head\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSqZtns6YdHP",
        "outputId": "31d135ea-2f64-4747-e748-a05912d78530"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 8, 256), dtype=float32, numpy=\n",
              "array([[[1., 1., 1., ..., 1., 1., 1.],\n",
              "        [1., 1., 1., ..., 1., 1., 1.],\n",
              "        [1., 1., 1., ..., 1., 1., 1.],\n",
              "        ...,\n",
              "        [1., 1., 1., ..., 1., 1., 1.],\n",
              "        [1., 1., 1., ..., 1., 1., 1.],\n",
              "        [1., 1., 1., ..., 1., 1., 1.]]], dtype=float32)>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# testing\n",
        "attention=CustomSelfAttention(256)\n",
        "attention(tf.ones([1,8,256]),tf.ones([1,8,256]),tf.ones([1,8,256]),padding_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Unruv2vNS6g"
      },
      "outputs": [],
      "source": [
        "class CustomMultiHeadAttention(Layer):\n",
        "  def __init__(self, num_heads, key_dim):\n",
        "      \"\"\"\n",
        "      Initialize the CustomMultiHeadAttention layer.\n",
        "      \n",
        "      Args:\n",
        "          num_heads (int): Number of attention heads.\n",
        "          key_dim (int): Dimension of the key, query, and value vectors.\n",
        "      \"\"\"\n",
        "      super(CustomMultiHeadAttention, self).__init__()\n",
        "\n",
        "      # Number of attention heads\n",
        "      self.num_heads = num_heads\n",
        "      \n",
        "      # Define dense layers for query, key, and value for each head\n",
        "      self.dense_q = [Dense(key_dim // num_heads) for _ in range(num_heads)]\n",
        "      self.dense_k = [Dense(key_dim // num_heads) for _ in range(num_heads)]\n",
        "      self.dense_v = [Dense(key_dim // num_heads) for _ in range(num_heads)]\n",
        "      \n",
        "      # Dense layer to project the concatenated heads back to the original key dimension\n",
        "      self.dense_o = Dense(key_dim)\n",
        "  \n",
        "      # Instance of the custom self-attention mechanism\n",
        "      self.self_attention = CustomSelfAttention(key_dim)\n",
        "\n",
        "\n",
        "  def call(self, query, key, value, attention_mask):\n",
        "      \"\"\"\n",
        "      Perform the multi-head attention operation.\n",
        "      \n",
        "      Args:\n",
        "          query (Tensor): Query matrix of shape (batch_size, seq_length, key_dim).\n",
        "          key (Tensor): Key matrix of shape (batch_size, seq_length, key_dim).\n",
        "          value (Tensor): Value matrix of shape (batch_size, seq_length, key_dim).\n",
        "          attention_mask (Tensor): Masking tensor of shape (batch_size, seq_length, seq_length),\n",
        "                                    where masked positions are set to 0 and unmasked to 1.\n",
        "      \n",
        "      Returns:\n",
        "          Tensor: The output of the multi-head attention layer of shape (batch_size, seq_length, key_dim).\n",
        "      \"\"\"\n",
        "      heads = []\n",
        "\n",
        "      # Loop through each head to compute self-attention for each\n",
        "      for i in range(self.num_heads):\n",
        "          # Apply dense layers to the query, key, and value for the current head\n",
        "          head = self.self_attention(self.dense_q[i](query), self.dense_k[i](key),\n",
        "                                      self.dense_v[i](value), attention_mask)\n",
        "          heads.append(head)\n",
        "      \n",
        "      # Concatenate the results from all heads along the last dimension\n",
        "      heads = tf.concat(heads, axis=2)\n",
        "      \n",
        "      # Apply the output dense layer to project back to the original key dimension\n",
        "      heads = self.dense_o(heads)\n",
        "      \n",
        "      return heads\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-Y512TzkOKQ"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jvze09LsnAw"
      },
      "outputs": [],
      "source": [
        "#?tf.keras.layers.MultiHeadAttention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5lTjZBE6M4Q"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads):\n",
        "        \"\"\"\n",
        "        Initialize the TransformerEncoder layer.\n",
        "        \n",
        "        Args:\n",
        "            embed_dim (int): Dimension of the embedding vectors.\n",
        "            dense_dim (int): Dimension of the dense projection layer.\n",
        "            num_heads (int): Number of attention heads.\n",
        "        \"\"\"\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        \n",
        "        # Multi-head attention layer\n",
        "        self.attention = CustomMultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        \n",
        "        # Sequential dense projection layer with ReLU activation followed by a linear layer\n",
        "        self.dense_proj = tf.keras.Sequential(\n",
        "            [Dense(dense_dim, activation=\"relu\"),\n",
        "             Dense(embed_dim)]\n",
        "        )\n",
        "        \n",
        "        # Layer normalization for the residual connections\n",
        "        self.layernorm_1 = LayerNormalization()\n",
        "        self.layernorm_2 = LayerNormalization()\n",
        "        \n",
        "        # Support masking in the encoder layer\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        \"\"\"\n",
        "        Perform the forward pass of the Transformer encoder layer.\n",
        "        \n",
        "        Args:\n",
        "            inputs (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n",
        "            mask (Tensor, optional): Optional masking tensor of shape (batch_size, seq_length) \n",
        "                                     to avoid attending to certain positions.\n",
        "        \n",
        "        Returns:\n",
        "            Tensor: The output tensor of the Transformer encoder layer of shape \n",
        "                    (batch_size, seq_length, embed_dim).\n",
        "        \"\"\"\n",
        "\n",
        "        # Prepare the padding mask for the attention mechanism, if provided\n",
        "        if mask is not None:\n",
        "            mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            T = tf.shape(mask)[2]\n",
        "            padding_mask = tf.repeat(mask, T, axis=1)\n",
        "        else:\n",
        "            padding_mask = None\n",
        "\n",
        "        # Apply the multi-head attention layer with the padding mask\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, key=inputs, value=inputs,\n",
        "            attention_mask=padding_mask\n",
        "        )\n",
        "\n",
        "        # Add and normalize the residual connection (input + attention output)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        \n",
        "        # Apply the dense projection layer\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        \n",
        "        # Add and normalize the residual connection (proj_input + proj_output)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOgozN6bu3hv",
        "outputId": "451a2e2d-40d4-4732-bdc7-ab4e6af5a1c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hello (1, 8, 64)\n",
            "hello (1, 8, 64)\n",
            "hello (1, 8, 64)\n",
            "hello (1, 8, 64)\n",
            "hello (1, 8, 64)\n",
            "hello (1, 8, 64)\n",
            "hello (1, 8, 64)\n",
            "hello (1, 8, 64)\n",
            "head (8, 1, 8, 64)\n",
            "(1, 8, 512)\n"
          ]
        }
      ],
      "source": [
        "# testing\n",
        "encoder_outputs = TransformerEncoder(512,2048,8)(emb_out)\n",
        "print(encoder_outputs.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDTmNxkmkctq"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdHTwxmfHWZW"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(Layer):\n",
        "  def __init__(self, embed_dim, latent_dim, num_heads):\n",
        "      \"\"\"\n",
        "      Initialize the TransformerDecoder layer.\n",
        "      \n",
        "      Args:\n",
        "          embed_dim (int): Dimension of the embedding vectors.\n",
        "          latent_dim (int): Dimension of the dense projection layer.\n",
        "          num_heads (int): Number of attention heads.\n",
        "      \"\"\"\n",
        "      super(TransformerDecoder, self).__init__()\n",
        "      self.embed_dim = embed_dim\n",
        "      self.latent_dim = latent_dim\n",
        "      self.num_heads = num_heads\n",
        "      \n",
        "      # First multi-head self-attention layer (attends to the decoder's own inputs)\n",
        "      self.attention_1 = MultiHeadAttention(\n",
        "          num_heads=num_heads, key_dim=embed_dim\n",
        "      )\n",
        "      \n",
        "      # Second multi-head attention layer (attends to the encoder's output)\n",
        "      self.attention_2 = MultiHeadAttention(\n",
        "          num_heads=num_heads, key_dim=embed_dim\n",
        "      )\n",
        "      \n",
        "      # Dense projection layer with ReLU activation followed by a linear layer\n",
        "      self.dense_proj = tf.keras.Sequential(\n",
        "          [Dense(latent_dim, activation=\"relu\"), Dense(embed_dim)]\n",
        "      )\n",
        "      \n",
        "      # Layer normalization layers for the residual connections\n",
        "      self.layernorm_1 = LayerNormalization()\n",
        "      self.layernorm_2 = LayerNormalization()\n",
        "      self.layernorm_3 = LayerNormalization()\n",
        "      \n",
        "      # Support masking in the decoder layer\n",
        "      self.supports_masking = True\n",
        "\n",
        "  def call(self, inputs, encoder_outputs, enc_mask, mask=None):\n",
        "      \"\"\"\n",
        "      Perform the forward pass of the Transformer decoder layer.\n",
        "      \n",
        "      Args:\n",
        "          inputs (Tensor): Input tensor (decoder input) of shape (batch_size, seq_length, embed_dim).\n",
        "          encoder_outputs (Tensor): Encoder output tensor of shape (batch_size, seq_length, embed_dim).\n",
        "          enc_mask (Tensor): Masking tensor for the encoder outputs of shape (batch_size, seq_length).\n",
        "          mask (Tensor, optional): Optional masking tensor for the decoder input of shape \n",
        "                                    (batch_size, seq_length) to avoid attending to certain positions.\n",
        "      \n",
        "      Returns:\n",
        "          Tensor: The output tensor of the Transformer decoder layer of shape \n",
        "                  (batch_size, seq_length, embed_dim).\n",
        "      \"\"\"\n",
        "\n",
        "      # Prepare the causal mask (prevents attending to future positions in the sequence)\n",
        "      if mask is not None:\n",
        "          causal_mask = tf.linalg.band_part(\n",
        "              tf.ones([tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[1]], dtype=tf.int32), -1, 0\n",
        "          )\n",
        "          \n",
        "          # Convert mask and encoder mask to the correct shape and type\n",
        "          mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "          enc_mask = tf.cast(enc_mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "          \n",
        "          # Create padding masks for the attention mechanisms\n",
        "          T = tf.shape(mask)[2]\n",
        "          padding_mask = tf.repeat(mask, T, axis=1)\n",
        "          cross_attn_mask = tf.repeat(enc_mask, T, axis=1)\n",
        "          \n",
        "          # Combine causal and padding masks\n",
        "          combined_mask = tf.minimum(padding_mask, causal_mask)\n",
        "      else:\n",
        "          combined_mask = None\n",
        "          cross_attn_mask = None\n",
        "\n",
        "      # First attention block: self-attention over the decoder's inputs\n",
        "      attention_output_1 = self.attention_1(\n",
        "          query=inputs, key=inputs, value=inputs,\n",
        "          attention_mask=combined_mask,\n",
        "      )\n",
        "      \n",
        "      # Add and normalize the residual connection (input + attention output)\n",
        "      out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "      \n",
        "      # Second attention block: attention over the encoder's outputs\n",
        "      attention_output_2 = self.attention_2(\n",
        "          query=out_1, key=encoder_outputs, value=encoder_outputs,\n",
        "          attention_mask=cross_attn_mask,\n",
        "      )\n",
        "      \n",
        "      # Add and normalize the residual connection (out_1 + attention output)\n",
        "      out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "      \n",
        "      # Apply the dense projection layer\n",
        "      proj_output = self.dense_proj(out_2)\n",
        "      \n",
        "      # Add and normalize the residual connection (out_2 + proj_output)\n",
        "      return self.layernorm_3(out_2 + proj_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bm6yX6fix8_4",
        "outputId": "b3d3116c-5d77-4d82-fcc9-d995e122f253"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "before tf.Tensor([[ True  True  True  True  True  True False False]], shape=(1, 8), dtype=bool) (1, 8)\n",
            "the boy tf.Tensor([[[1 1 1 1 1 1 0 0]]], shape=(1, 1, 8), dtype=int32) (1, 1, 8)\n",
            "(1, 8, 512)\n"
          ]
        }
      ],
      "source": [
        "enc_mask=mask\n",
        "decoder_outputs = TransformerDecoder(512,2048,4)(emb_out,encoder_outputs,enc_mask)\n",
        "print(decoder_outputs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SPdfBx8ke-Z"
      },
      "source": [
        "## Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsT7pvXfzh_E"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM=512\n",
        "D_FF=2048\n",
        "NUM_HEADS=8\n",
        "NUM_LAYERS=1\n",
        "NUM_EPOCHS=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7_vJmd8HWbu",
        "outputId": "73ac16d5-9cd9-419f-943f-c097b31756a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embeddings_3 (Embeddings)      (None, 32, 512)      10240000    ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embeddings_4 (Embeddings)      (None, 32, 512)      10240000    ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " transformer_encoder_3 (Transfo  (None, 32, 512)     10503168    ['embeddings_3[0][0]']           \n",
            " rmerEncoder)                                                                                     \n",
            "                                                                                                  \n",
            " tf.math.not_equal_1 (TFOpLambd  (None, None)        0           ['input_1[0][0]']                \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " transformer_decoder_3 (Transfo  (None, 32, 512)     18905600    ['embeddings_4[0][0]',           \n",
            " rmerDecoder)                                                     'transformer_encoder_3[0][0]',  \n",
            "                                                                  'tf.math.not_equal_1[0][0]']    \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 32, 512)      0           ['transformer_decoder_3[0][0]']  \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 32, 20000)    10260000    ['dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 60,148,768\n",
            "Trainable params: 60,148,768\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Define the input layer for the encoder (English input sequence)\n",
        "encoder_inputs = Input(shape=(None,), dtype=\"int64\", name=\"input_1\")\n",
        "# Create the embedding layer for the encoder input\n",
        "emb = Embeddings(ENGLISH_SEQUENCE_LENGTH, VOCAB_SIZE, EMBEDDING_DIM)\n",
        "# Embed the encoder inputs\n",
        "x = emb(encoder_inputs)\n",
        "# Compute the mask for the encoder inputs\n",
        "enc_mask = emb.compute_mask(encoder_inputs)\n",
        "\n",
        "# Apply the Transformer encoder layers sequentially\n",
        "for _ in range(NUM_LAYERS):\n",
        "    x = TransformerEncoder(EMBEDDING_DIM, D_FF, NUM_HEADS)(x)\n",
        "\n",
        "# Final encoder output after applying all encoder layers\n",
        "encoder_outputs = x\n",
        "\n",
        "\n",
        "\n",
        "# -------------------- decoder part ---------------\n",
        "\n",
        "# Define the input layer for the decoder (French input sequence)\n",
        "decoder_inputs = Input(shape=(None,), dtype=\"int64\", name=\"input_2\")\n",
        "# Embed the decoder inputs\n",
        "x = Embeddings(FRENCH_SEQUENCE_LENGTH, VOCAB_SIZE, EMBEDDING_DIM)(decoder_inputs)\n",
        "\n",
        "# Apply the Transformer decoder layers sequentially\n",
        "for i in range(NUM_LAYERS):\n",
        "    x = TransformerDecoder(EMBEDDING_DIM, D_FF, NUM_HEADS)(x, encoder_outputs, enc_mask)\n",
        "\n",
        "# Apply a dropout layer for regularization\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "# Apply the final dense layer with softmax activation to predict the output sequence\n",
        "decoder_outputs = Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "# Define the complete Transformer model with encoder and decoder inputs and outputs\n",
        "transformer = tf.keras.Model(\n",
        "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        ")\n",
        "\n",
        "\n",
        "# Print a summary of the Transformer model\n",
        "transformer.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urO3w5e971Hp"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BGwG2PlbZAB"
      },
      "outputs": [],
      "source": [
        "class Scheduler(LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps):\n",
        "        \"\"\"\n",
        "        Initialize the learning rate scheduler.\n",
        "        \n",
        "        Args:\n",
        "            d_model (int): Dimension of the model (usually the embedding dimension).\n",
        "            warmup_steps (int): Number of steps during which the learning rate is increased.\n",
        "        \"\"\"\n",
        "        super(Scheduler, self).__init__()\n",
        "        \n",
        "        # Cast the model dimension and warmup steps to float64 for precision\n",
        "        self.d_model = tf.cast(d_model, tf.float64)\n",
        "        self.warmup_steps = tf.cast(warmup_steps, dtype=tf.float64)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        \"\"\"\n",
        "        Compute the learning rate for a given training step.\n",
        "        \n",
        "        Args:\n",
        "            step (int or Tensor): The current training step.\n",
        "        \n",
        "        Returns:\n",
        "            Tensor: The computed learning rate for the given step.\n",
        "        \"\"\"\n",
        "        # Cast the current step to float64 for precision\n",
        "        step = tf.cast(step, dtype=tf.float64)\n",
        "        \n",
        "        # Calculate the learning rate using the formula from the \"Attention Is All You Need\" paper\n",
        "        learning_rate = (self.d_model**(-0.5)) * tf.math.minimum(\n",
        "            step**(-0.5), step * (self.warmup_steps ** -1.5)\n",
        "        )\n",
        "        \n",
        "        return learning_rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzXSQmVEeDsR"
      },
      "outputs": [],
      "source": [
        "WARM_UP_STEPS = 4000\n",
        "lr_scheduled = Scheduler(EMBEDDING_DIM, WARM_UP_STEPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAKCOX98HWd9"
      },
      "outputs": [],
      "source": [
        "transformer.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    optimizer = Adam(lr_scheduled)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2529VjOrVuGe",
        "outputId": "61d675e5-21c4-4b21-d487-252e028ccdd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1405/1405 [==============================] - 365s 237ms/step - loss: 5.4511 - val_loss: 4.7071\n",
            "Epoch 2/10\n",
            "1405/1405 [==============================] - 213s 151ms/step - loss: 3.3392 - val_loss: 3.7340\n",
            "Epoch 3/10\n",
            "1405/1405 [==============================] - 211s 149ms/step - loss: 2.6036 - val_loss: 3.3412\n",
            "Epoch 4/10\n",
            "1405/1405 [==============================] - 208s 148ms/step - loss: 2.1854 - val_loss: 3.0057\n",
            "Epoch 5/10\n",
            "1405/1405 [==============================] - 208s 148ms/step - loss: 1.9171 - val_loss: 2.8613\n",
            "Epoch 6/10\n",
            "1405/1405 [==============================] - 208s 148ms/step - loss: 1.7524 - val_loss: 2.7807\n",
            "Epoch 7/10\n",
            "1405/1405 [==============================] - 209s 148ms/step - loss: 1.6374 - val_loss: 2.7714\n",
            "Epoch 8/10\n",
            "1405/1405 [==============================] - 207s 147ms/step - loss: 1.5560 - val_loss: 2.7180\n",
            "Epoch 9/10\n",
            "1405/1405 [==============================] - 208s 148ms/step - loss: 1.4913 - val_loss: 2.7244\n",
            "Epoch 10/10\n",
            "1405/1405 [==============================] - 207s 147ms/step - loss: 1.4406 - val_loss: 2.7238\n"
          ]
        }
      ],
      "source": [
        "history=transformer.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RoBvoMzeSPx"
      },
      "outputs": [],
      "source": [
        "transformer.save_weights('transformers.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "rj3wLwSW7EMb",
        "outputId": "6e6b320c-526b-4c82-f8f1-09a052d192dc"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model_loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWJfUFU9VrQx",
        "outputId": "2b2a9972-d500-4850-afa0-988d349125e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "375/375 [==============================] - 53s 46ms/step - loss: 2.7233\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "2.7233364582061768"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformer.evaluate(val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i1ajAymaDoF"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5vMnTM62Cx_"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary mapping from index to word for the French vocabulary.\n",
        "# `index_to_word` will map each index (from 0 to the size of the vocabulary) to its corresponding word.\n",
        "# The vocabulary is obtained from the `french_vectorize_layer`.\n",
        "index_to_word = {x: y for x, y in zip(range(len(french_vectorize_layer.get_vocabulary())),\n",
        "                                      french_vectorize_layer.get_vocabulary())}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiWzshkMKfQg"
      },
      "outputs": [],
      "source": [
        "def translator(english_sentence):\n",
        "    \"\"\"\n",
        "    Translate an English sentence into French using the trained Transformer model.\n",
        "    \n",
        "    Args:\n",
        "        english_sentence (str): The input sentence in English.\n",
        "    \n",
        "    Returns:\n",
        "        str: The translated sentence in French.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the English sentence using the vectorization layer\n",
        "    tokenized_english_sentence = english_vectorize_layer([english_sentence])\n",
        "    \n",
        "    # Initialize the shifted target sequence with the start token\n",
        "    shifted_target = 'starttoken'\n",
        "    \n",
        "    # Loop to generate each word in the translated French sentence\n",
        "    for i in range(FRENCH_SEQUENCE_LENGTH):\n",
        "        # Tokenize the current shifted target sequence\n",
        "        tokenized_shifted_target = french_vectorize_layer([shifted_target])\n",
        "        \n",
        "        # Predict the next word using the Transformer model\n",
        "        output = transformer.predict([tokenized_english_sentence, tokenized_shifted_target])\n",
        "        \n",
        "        # Get the index of the predicted word with the highest probability\n",
        "        french_word_index = tf.argmax(output, axis=-1)[0][i].numpy()\n",
        "        \n",
        "        # Convert the word index to the corresponding French word\n",
        "        current_word = index_to_word[french_word_index]\n",
        "        \n",
        "        # Stop if the end token is predicted\n",
        "        if (current_word == 'endtoken'):\n",
        "            break\n",
        "        \n",
        "        # Append the predicted word to the shifted target sequence\n",
        "        shifted_target += ' ' + current_word\n",
        "    \n",
        "    # Return the translated sentence, excluding the 'starttoken'\n",
        "    return shifted_target[11:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "f1BQFbrdKfVZ",
        "outputId": "4d574f8e-5a3e-4bbd-ee4b-0f491c134cb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'astu déjà regardé au foot sous la pluie'"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translator('Have you ever watched soccer under the rain?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "je_d1YvagbZO",
        "outputId": "82fc31d3-5a9f-4b64-9fb7-ebcfcd926336"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'quel est ton nom de ton nom'"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translator(\"what is your name?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "BtuuqPkgVNB0",
        "outputId": "9692162c-a571-4b5f-f380-bbddf3672934"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'mon hôtel ma dit de vous appeler'"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translator('My hotel told me to call you. ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "fekzqiwGerZd",
        "outputId": "b328ceb0-11cf-4ab2-a513-bc04f63a3809"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'elle lui a donné largent de largent'"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translator('She handed him the money')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SFcAw__gv8B"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
